{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIpZZJnd26i0HjCqKsieUA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgeUfUv7s7XM"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eHigoYNtxXM"
      },
      "source": [
        "batch_size = 128\n",
        "total_words = 10000\n",
        "max_review_len = 80\n",
        "embedding_len = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-AtSIcGt1qE",
        "outputId": "b35c0072-4b14-40b4-a88b-8e6224449b98"
      },
      "source": [
        "# imdb.load_data 함수 사용하여 IMDB 데이터셋 다운\n",
        "# 파라미터 num_words는 데이터에서 등장 빈도 순위로 몇 번째에 해당하는 단어까지 사용할지를 의미\n",
        "# 등장 빈도 순위가 1 ~ 10000에 해당하는 단어만 사용\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words = total_words)\n",
        "\n",
        "'''\n",
        "전체 훈련 셋에서 각 샘플의 길이는 서로 다를수 있음\n",
        "또한 각 문장은 단어 수가 제 각각\n",
        "모델의 입력으로 사용하려면 모든 샘플 길이를 동일하게 맞추어야함\n",
        "이를 자연어 처리에서는 패딩작업이라고 함, 보통 숫자 0을 넣어서 길이를 맞춤\n",
        "케라스에서는 pad_sequence()를 사용\n",
        "- 첫 번쨰 인자 : 패딩을 진행할 데이터\n",
        "- maxlen : 모든 데이터에 대해 정규화 할 길이\n",
        "'''\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen = max_review_len)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen = max_review_len)\n",
        "\n",
        "'''\n",
        "넘파이 배열을 Dataset으로 변환\n",
        "변환하려는 전체 데이터를 메모리로 로딩해야 하므로 큰 용량의 메모리가 필요\n",
        "메모리 문제 해결책은 Dataset의 from_generator 사용\n",
        "이것은 필요할 떄만 파이썬 generator을 통해 가져옴\n",
        "'''\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "\n",
        "'''\n",
        "train_data.shuffle(10000) : \n",
        "- 데이터셋을 임의로 섞음, buffer_size 지정\n",
        "- 버퍼에서 임의로 샘플 뽑고, 뽑은 샘플은 다른 샘플로 대체\n",
        "- 데이터셋의 크기에 비해 크거나 같은 버퍼 크기로 지정\n",
        "\n",
        "bathch()\n",
        "- 몇 개의 샘플로 가중치를 갱신할지 지정\n",
        "\n",
        "drop_remainder\n",
        "- 마지막 배치 크기를 무시하고 지정한 배치 크기 사용\n",
        "'''\n",
        "train_data = train_data.shuffle(10000).batch(batch_size, drop_remainder = True)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_data = test_data.batch(batch_size, drop_remainder = True)\n",
        "\n",
        "print(\"X_train_shape,\", X_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\n",
        "print(\"X_test_shape\", X_test.shape)\n",
        "\n",
        "sample = next(iter(test_data))\n",
        "print(sample[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "X_train_shape, (25000, 80) tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "X_test_shape (25000, 80)\n",
            "(128, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E_2oQQV6gqC"
      },
      "source": [
        "# GRU 셀 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbCmoFfW6isB"
      },
      "source": [
        "class GRU_Build(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(GRU_Build, self).__init__()\n",
        "\n",
        "        self.state0 = [tf.zeros([batch_size, units])]\n",
        "        self.state1 = [tf.zeros([batch_size, units])]\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(total_words, embedding_len,\n",
        "                                                   input_length=max_review_len)\n",
        "        self.RNNCell0 = tf.keras.layers.GRUCell(units, dropout=0.5) ------ ①\n",
        "        self.RNNCell1 = tf.keras.layers.GRUCell(units, dropout=0.5)\n",
        "        self.outlayer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs\n",
        "        x = self.embedding(x)\n",
        "        state0 = self.state0 ------ 초기 상태는 모두 0으로 설정\n",
        "        state1 = self.state1\n",
        "        for word in tf.unstack(x, axis=1):\n",
        "            out0, state0 = self.RNNCell0(word, state0, training)\n",
        "            out1, state1 = self.RNNCell1(out0, state1, training)\n",
        "\n",
        "        x = self.outlayer(out1)\n",
        "        prob = tf.sigmoid(x)\n",
        "\n",
        "        return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gHaLA1L6ljx"
      },
      "source": [
        "units = 64\n",
        "epochs = 4\n",
        "t0 = time.time()\n",
        "\n",
        "model = GRU_Build(units)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss=tf.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'],\n",
        "              experimental_run_tf_function=False)\n",
        "\n",
        "model.fit(train_data, epochs=epochs, validation_data=test_data, validation_freq=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxyzbCwN6qix"
      },
      "source": [
        "# 평가\n",
        "print(\"훈련 데이터셋 평가...\")\n",
        "(loss, accuracy) = model.evaluate(train_data, verbose=0)\n",
        "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
        "print(\"테스트 데이터셋 평가...\")\n",
        "(loss, accuracy) = model.evaluate(test_data, verbose=0)\n",
        "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
        "t1 = time.time()\n",
        "print('시간:', t1-t0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8decU7N6uCJ"
      },
      "source": [
        "# GRU 계층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PhzdtYW6ve6"
      },
      "source": [
        "class GRU_Build(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(GRU_Build, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(total_words, embedding_len,\n",
        "                                                   input_length=max_review_len)\n",
        "        self.rnn = tf.keras.Sequential([\n",
        "            tf.keras.layers.GRU(units, dropout=0.5, return_sequences=True, unroll=True),\n",
        "            tf.keras.layers.GRU(units, dropout=0.5, unroll=True)\n",
        "        ])\n",
        "        self.outlayer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs\n",
        "        x = self.embedding(x)\n",
        "        x = self.rnn(x)\n",
        "        x = self.outlayer(x)\n",
        "        prob = tf.sigmoid(x)\n",
        "\n",
        "        return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLLc3-7q61I4"
      },
      "source": [
        "units = 64\n",
        "epochs = 4\n",
        "t0 = time.time()\n",
        "\n",
        "model = GRU_Build(units)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss=tf.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'],\n",
        "              experimental_run_tf_function=False)\n",
        "\n",
        "model.fit(train_data, epochs=epochs, validation_data=test_data, validation_freq=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhvnfeOx626p"
      },
      "source": [
        "print(\"훈련 데이터셋 평가...\")\n",
        "(loss, accuracy) = model.evaluate(train_data, verbose=0)\n",
        "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
        "print(\"테스트 데이터셋 평가...\")\n",
        "(loss, accuracy) = model.evaluate(test_data, verbose=0)\n",
        "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
        "\n",
        "t1 = time.time()\n",
        "print('시간:', t1-t0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
