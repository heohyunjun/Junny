{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmrLIwfDWWAy/PGhb1ZVQS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgeUfUv7s7XM"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eHigoYNtxXM"
      },
      "source": [
        "batch_size = 128\n",
        "total_words = 10000\n",
        "max_review_len = 80\n",
        "embedding_len = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-AtSIcGt1qE",
        "outputId": "b35c0072-4b14-40b4-a88b-8e6224449b98"
      },
      "source": [
        "# imdb.load_data 함수 사용하여 IMDB 데이터셋 다운\n",
        "# 파라미터 num_words는 데이터에서 등장 빈도 순위로 몇 번째에 해당하는 단어까지 사용할지를 의미\n",
        "# 등장 빈도 순위가 1 ~ 10000에 해당하는 단어만 사용\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words = total_words)\n",
        "\n",
        "'''\n",
        "전체 훈련 셋에서 각 샘플의 길이는 서로 다를수 있음\n",
        "또한 각 문장은 단어 수가 제 각각\n",
        "모델의 입력으로 사용하려면 모든 샘플 길이를 동일하게 맞추어야함\n",
        "이를 자연어 처리에서는 패딩작업이라고 함, 보통 숫자 0을 넣어서 길이를 맞춤\n",
        "케라스에서는 pad_sequence()를 사용\n",
        "- 첫 번쨰 인자 : 패딩을 진행할 데이터\n",
        "- maxlen : 모든 데이터에 대해 정규화 할 길이\n",
        "'''\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen = max_review_len)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen = max_review_len)\n",
        "\n",
        "'''\n",
        "넘파이 배열을 Dataset으로 변환\n",
        "변환하려는 전체 데이터를 메모리로 로딩해야 하므로 큰 용량의 메모리가 필요\n",
        "메모리 문제 해결책은 Dataset의 from_generator 사용\n",
        "이것은 필요할 떄만 파이썬 generator을 통해 가져옴\n",
        "'''\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "\n",
        "'''\n",
        "train_data.shuffle(10000) : \n",
        "- 데이터셋을 임의로 섞음, buffer_size 지정\n",
        "- 버퍼에서 임의로 샘플 뽑고, 뽑은 샘플은 다른 샘플로 대체\n",
        "- 데이터셋의 크기에 비해 크거나 같은 버퍼 크기로 지정\n",
        "\n",
        "bathch()\n",
        "- 몇 개의 샘플로 가중치를 갱신할지 지정\n",
        "\n",
        "drop_remainder\n",
        "- 마지막 배치 크기를 무시하고 지정한 배치 크기 사용\n",
        "'''\n",
        "train_data = train_data.shuffle(10000).batch(batch_size, drop_remainder = True)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_data = test_data.batch(batch_size, drop_remainder = True)\n",
        "\n",
        "print(\"X_train_shape,\", X_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\n",
        "print(\"X_test_shape\", X_test.shape)\n",
        "\n",
        "sample = next(iter(test_data))\n",
        "print(sample[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "X_train_shape, (25000, 80) tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "X_test_shape (25000, 80)\n",
            "(128, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdyCUB3mvFdL"
      },
      "source": [
        "# LSTM 셀 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6wJzP5hrxXj"
      },
      "source": [
        "class LSTM_Build(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(LSTM_Build, self).__init__()\n",
        "\n",
        "        self.state0 = [tf.zeros([batch_size, units]), tf.zeros([batch_size, units])]\n",
        "        self.state1 = [tf.zeros([batch_size, units]), tf.zeros([batch_size, units])]\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(total_words, embedding_len,\n",
        "        \n",
        "                                                   input_length=max_review_len)\n",
        "        \n",
        "        '''\n",
        "        - 첫 번쨰 인자 :  메모리 셀의 개수\n",
        "        - dropout : 전체 가중치 중 50% 값을 0으로 설정하여 사용하지 않겠다는 의미\n",
        "        '''\n",
        "        self.RNNCell0 = tf.keras.layers.LSTMCell(units, dropout=0.5)\n",
        "        self.RNNCell1 = tf.keras.layers.LSTMCell(units, dropout=0.5)\n",
        "        self.outlayer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs\n",
        "        x = self.embedding(x)\n",
        "        state0 = self.state0\n",
        "        state1 = self.state1\n",
        "        for word in tf.unstack(x, axis=1):\n",
        "            out0, state0 = self.RNNCell0(word, state0, training)\n",
        "            out1, state1 = self.RNNCell1(out0, state1, training)\n",
        "\n",
        "        x = self.outlayer(out1)\n",
        "        prob = tf.sigmoid(x)\n",
        "\n",
        "        return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nesZ-oMTs9k7",
        "outputId": "23edcd1c-1391-4179-df90-53e9f544ca8b"
      },
      "source": [
        "\n",
        "units = 64\n",
        "epochs = 4\n",
        "t0 = time.time()\n",
        "\n",
        "model = LSTM_Build(units)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss=tf.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'],\n",
        "              experimental_run_tf_function=False)\n",
        "\n",
        "model.fit(train_data, epochs=epochs, validation_data=test_data, validation_freq=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "195/195 [==============================] - 56s 202ms/step - loss: 0.4855 - accuracy: 0.7482\n",
            "Epoch 2/4\n",
            "195/195 [==============================] - 54s 277ms/step - loss: 0.3113 - accuracy: 0.8704 - val_loss: 0.3604 - val_accuracy: 0.8379\n",
            "Epoch 3/4\n",
            "195/195 [==============================] - 39s 198ms/step - loss: 0.2620 - accuracy: 0.8930\n",
            "Epoch 4/4\n",
            "195/195 [==============================] - 48s 245ms/step - loss: 0.2158 - accuracy: 0.9175 - val_loss: 0.4203 - val_accuracy: 0.8348\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff48cc67810>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hihGAnTCu1n7",
        "outputId": "2ce70e71-4d44-442a-8dbc-4dbc15bf9746"
      },
      "source": [
        "print(\"훈련 데이터셋 평가...\")\n",
        "(loss, accuracy) = model.evaluate(train_data, verbose=0)\n",
        "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
        "print(\"테스트 데이터셋 평가...\")\n",
        "(loss, accuracy) = model.evaluate(test_data, verbose=0)\n",
        "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
        "t1 = time.time()\n",
        "print('시간:', t1-t0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터셋 평가...\n",
            "loss=0.1328, accuracy: 95.7252%\n",
            "테스트 데이터셋 평가...\n",
            "loss=0.4203, accuracy: 83.4816%\n",
            "시간: 247.36914801597595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFhOKqzDvHq6"
      },
      "source": [
        "# LSTM 계층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJtmQxL4vJCS"
      },
      "source": [
        "class LSTM_Build(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(LSTM_Build, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(total_words, embedding_len,\n",
        "                                                   input_length=max_review_len)\n",
        "        self.rnn = tf.keras.Sequential([\n",
        "  \n",
        "            # units : 네트워크의 층 수(출력 공간 차원)\n",
        "            # return_sequences = True : 마지막 출력 또는 전체 순서를 반환\n",
        "            # 이떄 return sequences = False는 마지막 셀에서 밀집층이 한번만 적용\n",
        "            # unroll : 시간 순서에 따라 입력층과 은닉층에 대한 네트워크를 펼침\n",
        "            # 메모리 사용률은 높을 수 있찌만 계속 속도는 빨라질수있음\n",
        "            \n",
        "            tf.keras.layers.LSTM(units, dropout=0.5, return_sequences=True,\n",
        "                                 unroll=True),\n",
        "            tf.keras.layers.LSTM(units, dropout=0.5, unroll=True)\n",
        "        ])\n",
        "        self.outlayer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs\n",
        "        x = self.embedding(x)\n",
        "        x = self.rnn(x)\n",
        "        x = self.outlayer(x)\n",
        "        prob = tf.sigmoid(x)\n",
        "\n",
        "        return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4VyTK9jwUOJ",
        "outputId": "064acec0-85a4-4bf1-cb54-9b08aec2b414"
      },
      "source": [
        "units = 64\n",
        "epochs = 4\n",
        "t0 = time.time()\n",
        "\n",
        "model = LSTM_Build(units)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss=tf.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'],\n",
        "              experimental_run_tf_function=False)\n",
        "\n",
        "model.fit(train_data, epochs=epochs, validation_data=test_data, validation_freq=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "195/195 [==============================] - 58s 206ms/step - loss: 0.4907 - accuracy: 0.7452\n",
            "Epoch 2/4\n",
            "195/195 [==============================] - 57s 295ms/step - loss: 0.3129 - accuracy: 0.8707 - val_loss: 0.3597 - val_accuracy: 0.8389\n",
            "Epoch 3/4\n",
            "195/195 [==============================] - 41s 208ms/step - loss: 0.2566 - accuracy: 0.8972\n",
            "Epoch 4/4\n",
            "195/195 [==============================] - 55s 282ms/step - loss: 0.2206 - accuracy: 0.9145 - val_loss: 0.3937 - val_accuracy: 0.8334\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff4950c0810>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-vsdRaY18yl",
        "outputId": "b93c0535-02d4-4bef-cc01-6e98b2b12b25"
      },
      "source": [
        "print(\"훈련 데이터셋 평가...\")\n",
        "(loss, accuracy) = model.evaluate(train_data, verbose=0)\n",
        "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
        "print(\"테스트 데이터셋 평가...\")\n",
        "(loss, accuracy) = model.evaluate(test_data, verbose=0)\n",
        "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
        "\n",
        "t1 = time.time()\n",
        "print('시간:', t1-t0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터셋 평가...\n",
            "loss=0.1605, accuracy: 95.0761%\n",
            "테스트 데이터셋 평가...\n",
            "loss=0.3937, accuracy: 83.3413%\n",
            "시간: 1501.2773988246918\n"
          ]
        }
      ]
    }
  ]
}
